import streamlit as st
import openai
from datetime import datetime
import os
from dotenv import load_dotenv
from .error_handler import handle_error, LLMConnectionError, safe_execute_with_retry

# Load environment variables
load_dotenv()

class ChatHandler:
    def __init__(self):
        # Khá»Ÿi táº¡o Local LLM client (LM Studio)
        self.local_llm_url = os.getenv("LOCAL_LLM_URL", "http://localhost:1234/v1")
        try:
            self.client = openai.OpenAI(
                base_url=self.local_llm_url,
                api_key="not_needed"
            )
            self.api_key = "local_connected"  # Äá»ƒ tÆ°Æ¡ng thÃ­ch vá»›i logic cÅ©
            st.success("âœ… ChatHandler Ä‘Ã£ káº¿t ná»‘i Local LLM (LM Studio)")
        except Exception as e:
            self.client = None
            self.api_key = None
            st.warning(f"âš ï¸ ChatHandler khÃ´ng thá»ƒ káº¿t ná»‘i Local LLM: {str(e)}. á»¨ng dá»¥ng sáº½ cháº¡y á»Ÿ cháº¿ Ä‘á»™ demo.")
    
    def generate_response(self, user_input, chat_history, document_context="", is_document_qa=False):
        """Táº¡o pháº£n há»“i tá»« AI vá»›i enhanced document Q&A support vÃ  improved error handling"""
        
        # Náº¿u khÃ´ng cÃ³ API key, tráº£ vá» pháº£n há»“i demo
        if not self.api_key:
            return self._generate_demo_response(user_input, document_context)
        
        def _call_llm():
            # Chuáº©n bá»‹ messages cho OpenAI vá»›i enhanced context management
            if is_document_qa and document_context:
                # Specialized system prompt cho document Q&A
                system_content = self._create_document_qa_prompt(document_context)
                max_tokens = 600  # TÄƒng Ä‘á»ƒ cÃ³ cÃ¢u tráº£ lá»i chi tiáº¿t hÆ¡n
                temperature = 0.15  # Giáº£m Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c
            else:
                # Standard chat system prompt vá»›i context intelligence
                system_content = self._create_enhanced_system_prompt(document_context, user_input)
                max_tokens = 1200  # TÄƒng Ä‘á»ƒ cÃ³ pháº£n há»“i phong phÃº hÆ¡n
                temperature = 0.6  # CÃ¢n báº±ng giá»¯a creativity vÃ  consistency
            
            messages = [
                {
                    "role": "system",
                    "content": system_content
                }
            ]
            
            # Enhanced chat history management vá»›i intelligent filtering
            recent_history = self._filter_relevant_history(chat_history, user_input, max_messages=12)
            for msg in recent_history:
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            # ThÃªm tin nháº¯n hiá»‡n táº¡i vá»›i context enhancement
            enhanced_input = self._enhance_user_input(user_input, document_context)
            messages.append({
                "role": "user",
                "content": enhanced_input
            })
            
            # Gá»i Local LLM API vá»›i optimized parameters
            response = self.client.chat.completions.create(
                model="local-model",
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,  # ThÃªm top_p Ä‘á»ƒ kiá»ƒm soÃ¡t diversity
                frequency_penalty=0.1,  # Giáº£m repetition
                presence_penalty=0.1   # Encourage new topics
            )
            
            return response.choices[0].message.content
        
        # Sá»­ dá»¥ng safe_execute_with_retry Ä‘á»ƒ xá»­ lÃ½ lá»—i
        success, result, error_message = safe_execute_with_retry(
            _call_llm,
            max_retries=2,  # TÄƒng lÃªn 2 láº§n thá»­
            delay=1.0,      # TÄƒng delay Ä‘á»ƒ model cÃ³ thá»i gian
            context="Enhanced Local LLM Chat",
            show_user=False
        )
        
        if success:
            # Post-process response Ä‘á»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng
            return self._post_process_response(result, user_input, is_document_qa)
        else:
            # Xá»­ lÃ½ cÃ¡c loáº¡i lá»—i cá»¥ thá»ƒ vá»›i enhanced fallback
            return self._handle_llm_error(error_message, user_input, document_context)
    
    def _create_enhanced_system_prompt(self, document_context, user_input):
        """Táº¡o system prompt thÃ´ng minh cho chat vá»›i context awareness"""
        base_prompt = """Báº¡n lÃ  Study Buddy - má»™t AI assistant thÃ´ng minh chuyÃªn há»— trá»£ há»c táº­p. 

KHáº¢ NÄ‚NG Cá»T LÃ•I:
- ğŸ“š PhÃ¢n tÃ­ch vÃ  giáº£i thÃ­ch tÃ i liá»‡u há»c táº­p
- ğŸ¤” Tráº£ lá»i cÃ¢u há»i chi tiáº¿t vÃ  chÃ­nh xÃ¡c  
- ğŸ’¡ ÄÆ°a ra gá»£i Ã½ há»c táº­p thÃ´ng minh
- ğŸ¯ TÆ°Æ¡ng tÃ¡c tá»± nhiÃªn báº±ng tiáº¿ng Viá»‡t

NGUYÃŠN Táº®C HOáº T Äá»˜NG:
âœ… Tráº£ lá»i chÃ­nh xÃ¡c, cÃ³ cÄƒn cá»©
âœ… Giáº£i thÃ­ch dá»… hiá»ƒu, logic rÃµ rÃ ng
âœ… ÄÆ°a ra vÃ­ dá»¥ cá»¥ thá»ƒ khi cáº§n
âœ… Khuyáº¿n khÃ­ch tÆ° duy pháº£n biá»‡n
âŒ KhÃ´ng bá»‹a Ä‘áº·t thÃ´ng tin sai lá»‡ch
âŒ KhÃ´ng tráº£ lá»i nhá»¯ng cÃ¢u há»i khÃ´ng phÃ¹ há»£p"""
        
        if document_context:
            # Intelligent context inclusion dá»±a trÃªn user input
            context_snippet = self._extract_relevant_context(document_context, user_input)
            base_prompt += f"""

ğŸ“„ NGá»® Cáº¢NH TÃ€I LIá»†U:
NgÆ°á»i dÃ¹ng Ä‘Ã£ upload tÃ i liá»‡u. DÆ°á»›i Ä‘Ã¢y lÃ  pháº§n ná»™i dung liÃªn quan:

{context_snippet}

HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG TÃ€I LIá»†U:
- Æ¯u tiÃªn thÃ´ng tin tá»« tÃ i liá»‡u khi tráº£ lá»i
- TrÃ­ch dáº«n cá»¥ thá»ƒ khi cáº§n thiáº¿t
- Ghi rÃµ nguá»“n: "Theo tÃ i liá»‡u báº¡n upload..."
- Káº¿t há»£p kiáº¿n thá»©c chung khi phÃ¹ há»£p"""
        
        return base_prompt

    def _create_system_prompt(self, document_context):
        """Backward compatibility method"""
        return self._create_enhanced_system_prompt(document_context, "")
    
    def _create_document_qa_prompt(self, document_context):
        """Táº¡o system prompt chuyÃªn biá»‡t cho document Q&A"""
        return f"""Báº¡n lÃ  má»™t AI assistant chuyÃªn tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p.

NHIá»†M Vá»¤:
- Tráº£ lá»i cÃ¢u há»i chÃ­nh xÃ¡c vÃ  chi tiáº¿t dá»±a trÃªn ná»™i dung tÃ i liá»‡u
- Chá»‰ sá»­ dá»¥ng thÃ´ng tin cÃ³ trong tÃ i liá»‡u
- Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, hÃ£y nÃ³i rÃµ
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, sÃºc tÃ­ch vÃ  dá»… hiá»ƒu

NGUYÃŠN Táº®C:
âœ… Dá»±a hoÃ n toÃ n vÃ o ná»™i dung tÃ i liá»‡u
âœ… TrÃ­ch dáº«n cá»¥ thá»ƒ khi cáº§n thiáº¿t  
âœ… Thá»«a nháº­n khi khÃ´ng cÃ³ thÃ´ng tin
âŒ KhÃ´ng bá»‹a Ä‘áº·t thÃ´ng tin khÃ´ng cÃ³ trong tÃ i liá»‡u
âŒ KhÃ´ng suy diá»…n quÃ¡ xa

TÃ€I LIá»†U THAM KHáº¢O:
{document_context[:2500]}

HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u trÃªn."""
    
    def _generate_demo_response(self, user_input, document_context):
        """Táº¡o pháº£n há»“i demo khi khÃ´ng cÃ³ Local LLM connection"""
        if document_context:
            return f"""ğŸ¤– **Pháº£n há»“i Demo vá»›i TÃ i liá»‡u**

CÃ¢u há»i cá»§a báº¡n: "{user_input}"

Dá»±a trÃªn tÃ i liá»‡u báº¡n Ä‘Ã£ upload, tÃ´i cÃ³ thá»ƒ tháº¥y:
- TÃ i liá»‡u chá»©a {len(document_context)} kÃ½ tá»±
- Ná»™i dung liÃªn quan Ä‘áº¿n cÃ¢u há»i cá»§a báº¡n

**LÆ°u Ã½:** ÄÃ¢y lÃ  cháº¿ Ä‘á»™ demo. Äá»ƒ cÃ³ tráº£i nghiá»‡m Ä‘áº§y Ä‘á»§ vá»›i Local LLM:

**CÃ¡ch kÃ­ch hoáº¡t:**
1. ğŸš€ Khá»Ÿi Ä‘á»™ng LM Studio trÃªn port 1234
2. ğŸ“ Load má»™t model báº¥t ká»³ (llama, mistral, etc.)
3. â–¶ï¸ Restart á»©ng dá»¥ng Streamlit

Preview ná»™i dung tÃ i liá»‡u: {document_context[:200]}..."""
        
        return f"""ğŸ¤– **Pháº£n há»“i Demo - Local LLM**

CÃ¢u há»i cá»§a báº¡n: "{user_input}"

Xin chÃ o! TÃ´i lÃ  Study Buddy vá»›i Local LLM support. Hiá»‡n táº¡i Ä‘ang cháº¡y á»Ÿ cháº¿ Ä‘á»™ demo.

**ğŸ¯ Äá»ƒ demo hoáº¡t Ä‘á»™ng Ä‘áº§y Ä‘á»§:**

**BÆ°á»›c 1:** Khá»Ÿi Ä‘á»™ng LM Studio
- Download LM Studio tá»«: https://lmstudio.ai
- Má»Ÿ LM Studio vÃ  start server trÃªn port 1234

**BÆ°á»›c 2:** Load Model
- Download má»™t model (llama, mistral, phi, v.v.)
- Load model vÃ  start local server

**BÆ°á»›c 3:** Restart App
- Restart á»©ng dá»¥ng Streamlit nÃ y
- Báº¡n sáº½ tháº¥y "âœ… ÄÃ£ káº¿t ná»‘i Local LLM"

**ğŸ’¡ Lá»£i Ã­ch Local LLM:**
- âœ… HoÃ n toÃ n miá»…n phÃ­
- âœ… Cháº¡y offline 
- âœ… Privacy tuyá»‡t Ä‘á»‘i
- âœ… Demo á»•n Ä‘á»‹nh

Thá»i gian: {datetime.now().strftime('%H:%M:%S %d/%m/%Y')}"""
    
    def _handle_llm_error(self, error_message, user_input, document_context):
        """Xá»­ lÃ½ cÃ¡c loáº¡i lá»—i LLM cá»¥ thá»ƒ vÃ  tráº£ vá» pháº£n há»“i thÃ¢n thiá»‡n"""
        
        # Xá»­ lÃ½ lá»—i 404 - No models loaded
        if "404" in str(error_message) and "model_not_found" in str(error_message):
            return f"""âš ï¸ **Local LLM chÆ°a sáºµn sÃ ng**

CÃ¢u há»i cá»§a báº¡n: "{user_input}"

**ğŸ”§ NguyÃªn nhÃ¢n:** LM Studio chÆ°a load model nÃ o.

**ğŸ’¡ CÃ¡ch kháº¯c phá»¥c:**
1. ğŸš€ Má»Ÿ LM Studio
2. ğŸ“ Táº£i vÃ  load má»™t model (vÃ­ dá»¥: Llama, Mistral, Phi)
3. â–¶ï¸ Start local server trÃªn port 1234
4. ğŸ”„ Refresh láº¡i trang nÃ y

**ğŸ“‹ HÆ°á»›ng dáº«n chi tiáº¿t:**
- Download LM Studio: https://lmstudio.ai
- Chá»n tab "Chat" â†’ Load model â†’ Start server
- Äáº£m báº£o server cháº¡y trÃªn `http://localhost:1234`

**ğŸ¯ Tráº¡ng thÃ¡i hiá»‡n táº¡i:** Cháº¿ Ä‘á»™ chá» Local LLM
â° {datetime.now().strftime('%H:%M:%S %d/%m/%Y')}"""

        # Xá»­ lÃ½ lá»—i connection
        elif "connection" in str(error_message).lower() or "connect" in str(error_message).lower():
            return f"""ğŸ”Œ **Lá»—i káº¿t ná»‘i Local LLM**

CÃ¢u há»i cá»§a báº¡n: "{user_input}"

**ğŸ”§ NguyÃªn nhÃ¢n:** KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘áº¿n LM Studio server.

**ğŸ’¡ CÃ¡ch kháº¯c phá»¥c:**
1. âœ… Kiá»ƒm tra LM Studio Ä‘Ã£ cháº¡y chÆ°a
2. ğŸŒ Äáº£m báº£o server trÃªn port 1234
3. ğŸ”’ Táº¯t firewall/antivirus táº¡m thá»i
4. ğŸ”„ Restart LM Studio vÃ  á»©ng dá»¥ng nÃ y

**ğŸ“Š ThÃ´ng tin ká»¹ thuáº­t:**
- URL: {self.local_llm_url}
- Tráº¡ng thÃ¡i: Máº¥t káº¿t ná»‘i

â° {datetime.now().strftime('%H:%M:%S %d/%m/%Y')}"""

        # Xá»­ lÃ½ lá»—i timeout
        elif "timeout" in str(error_message).lower():
            return f"""â±ï¸ **Local LLM pháº£n há»“i cháº­m**

CÃ¢u há»i cá»§a báº¡n: "{user_input}"

**ğŸ”§ NguyÃªn nhÃ¢n:** Model xá»­ lÃ½ quÃ¡ lÃ¢u hoáº·c server quÃ¡ táº£i.

**ğŸ’¡ Gá»£i Ã½:**
1. ğŸ”„ Thá»­ láº¡i vá»›i cÃ¢u há»i ngáº¯n hÆ¡n
2. âš¡ Chá»n model nhá» hÆ¡n trong LM Studio
3. ğŸ–¥ï¸ ÄÃ³ng cÃ¡c á»©ng dá»¥ng khÃ¡c Ä‘á»ƒ giáº£i phÃ³ng RAM
4. â° Chá» má»™t chÃºt rá»“i thá»­ láº¡i

**ğŸ“Š Model hiá»‡n táº¡i cÃ³ thá»ƒ quÃ¡ lá»›n cho mÃ¡y nÃ y**

â° {datetime.now().strftime('%H:%M:%S %d/%m/%Y')}"""
        
        # Lá»—i general fallback
        else:
            return f"""ğŸ¤– **Táº¡m thá»i khÃ´ng thá»ƒ pháº£n há»“i**

CÃ¢u há»i cá»§a báº¡n: "{user_input}"

**ğŸ”§ Äang gáº·p sá»± cá»‘ ká»¹ thuáº­t vá»›i Local LLM**

**ğŸ’¡ CÃ³ thá»ƒ thá»­:**
1. ğŸ”„ Thá»­ láº¡i sau vÃ i giÃ¢y
2. ğŸš€ Restart LM Studio
3. ğŸ“± Kiá»ƒm tra model Ä‘Ã£ load Ä‘Ãºng chÆ°a
4. ğŸ’» Restart á»©ng dá»¥ng nÃ y

**ğŸ¯ Cháº¿ Ä‘á»™ dá»± phÃ²ng:** Demo mode
- á»¨ng dá»¥ng váº«n hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng
- Local LLM sáº½ tá»± phá»¥c há»“i khi sáºµn sÃ ng

â° {datetime.now().strftime('%H:%M:%S %d/%m/%Y')}
ğŸ”§ Lá»—i: {str(error_message)[:100]}..."""

    def _filter_relevant_history(self, chat_history, user_input, max_messages=12):
        """Lá»c lá»‹ch sá»­ chat cÃ³ liÃªn quan Ä‘áº¿n cÃ¢u há»i hiá»‡n táº¡i"""
        if not chat_history or len(chat_history) <= max_messages:
            return chat_history
        
        # Láº¥y keywords tá»« user input
        keywords = user_input.lower().split()
        relevant_messages = []
        
        # Æ¯u tiÃªn tin nháº¯n gáº§n Ä‘Ã¢y
        recent_messages = chat_history[-max_messages//2:]
        relevant_messages.extend(recent_messages)
        
        # TÃ¬m tin nháº¯n cÃ³ liÃªn quan tá»« lá»‹ch sá»­ cÅ© hÆ¡n
        older_messages = chat_history[:-max_messages//2]
        for msg in reversed(older_messages):
            if len(relevant_messages) >= max_messages:
                break
            
            # Kiá»ƒm tra keywords trong ná»™i dung
            msg_content = msg.get("content", "").lower()
            if any(keyword in msg_content for keyword in keywords if len(keyword) > 3):
                relevant_messages.insert(0, msg)
        
        return relevant_messages[-max_messages:]
    
    def _extract_relevant_context(self, document_context, user_input, max_length=2000):
        """TrÃ­ch xuáº¥t pháº§n context liÃªn quan Ä‘áº¿n cÃ¢u há»i"""
        if not document_context or not user_input:
            return document_context[:max_length]
        
        # TÃ¬m keywords trong user input
        keywords = [word.lower() for word in user_input.split() if len(word) > 3]
        
        # Chia document thÃ nh chunks
        chunks = []
        chunk_size = 500
        for i in range(0, len(document_context), chunk_size):
            chunk = document_context[i:i+chunk_size]
            chunks.append(chunk)
        
        # TÃ­nh score cho má»—i chunk
        chunk_scores = []
        for chunk in chunks:
            score = 0
            chunk_lower = chunk.lower()
            for keyword in keywords:
                score += chunk_lower.count(keyword)
            chunk_scores.append((chunk, score))
        
        # Sáº¯p xáº¿p vÃ  láº¥y chunks cÃ³ score cao
        chunk_scores.sort(key=lambda x: x[1], reverse=True)
        
        relevant_text = ""
        current_length = 0
        
        for chunk, score in chunk_scores:
            if current_length + len(chunk) <= max_length:
                relevant_text += chunk + "\n"
                current_length += len(chunk)
            else:
                break
        
        return relevant_text.strip() or document_context[:max_length]
    
    def _enhance_user_input(self, user_input, document_context):
        """Cáº£i thiá»‡n user input vá»›i context hints"""
        if not document_context:
            return user_input
        
        # ThÃªm context hint náº¿u user input quÃ¡ ngáº¯n
        if len(user_input.split()) <= 3:
            return f"{user_input}\n\n(Gá»£i Ã½: Tham kháº£o tÃ i liá»‡u Ä‘Ã£ upload Ä‘á»ƒ tráº£ lá»i chi tiáº¿t hÆ¡n)"
        
        return user_input
    
    def _post_process_response(self, response, user_input, is_document_qa):
        """Xá»­ lÃ½ vÃ  cáº£i thiá»‡n pháº£n há»“i tá»« LLM"""
        if not response:
            return "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ táº¡o pháº£n há»“i phÃ¹ há»£p. Báº¡n cÃ³ thá»ƒ thá»­ láº¡i vá»›i cÃ¢u há»i khÃ¡c khÃ´ng?"
        
        # Loáº¡i bá» cÃ¡c kÃ½ tá»± khÃ´ng mong muá»‘n
        response = response.strip()
        
        # ThÃªm emoji vÃ  formatting cho document Q&A
        if is_document_qa and not response.startswith(("ğŸ“„", "ğŸ¤–", "âœ…", "âŒ", "âš ï¸")):
            response = f"ğŸ“„ **Dá»±a trÃªn tÃ i liá»‡u:** {response}"
        
        # Äáº£m báº£o pháº£n há»“i khÃ´ng quÃ¡ dÃ i
        if len(response) > 2000:
            response = response[:1900] + "\n\n...(Pháº£n há»“i Ä‘Ã£ Ä‘Æ°á»£c rÃºt gá»n)"
        
        return response
