import streamlit as st
import openai
from datetime import datetime
import os
from dotenv import load_dotenv
from .error_handler import handle_error, LLMConnectionError, safe_execute_with_retry

# Load environment variables
load_dotenv()

class ChatHandler:
    def __init__(self):
        # Khá»Ÿi táº¡o Local LLM client (LM Studio)
        self.local_llm_url = os.getenv("LOCAL_LLM_URL", "http://localhost:1234/v1")
        try:
            self.client = openai.OpenAI(
                base_url=self.local_llm_url,
                api_key="not_needed"
            )
            self.api_key = "local_connected"  # Äá»ƒ tÆ°Æ¡ng thÃ­ch vá»›i logic cÅ©
            st.success("âœ… ChatHandler Ä‘Ã£ káº¿t ná»‘i Local LLM (LM Studio)")
        except Exception as e:
            self.client = None
            self.api_key = None
            st.warning(f"âš ï¸ ChatHandler khÃ´ng thá»ƒ káº¿t ná»‘i Local LLM: {str(e)}. á»¨ng dá»¥ng sáº½ cháº¡y á»Ÿ cháº¿ Ä‘á»™ demo.")
    
    def generate_response(self, user_input, chat_history, document_context="", is_document_qa=False):
        """Táº¡o pháº£n há»“i tá»« AI vá»›i enhanced document Q&A support vÃ  improved error handling"""
        
        # Náº¿u khÃ´ng cÃ³ API key, tráº£ vá» pháº£n há»“i demo
        if not self.api_key:
            return self._generate_demo_response(user_input, document_context)
        
        def _call_llm():
            # Chuáº©n bá»‹ messages cho OpenAI
            if is_document_qa and document_context:
                # Specialized system prompt cho document Q&A
                system_content = self._create_document_qa_prompt(document_context)
                max_tokens = 500  # Focused answers
                temperature = 0.2  # More deterministic
            else:
                # Standard chat system prompt
                system_content = self._create_system_prompt(document_context)
                max_tokens = 1000
                temperature = 0.7
            
            messages = [
                {
                    "role": "system",
                    "content": system_content
                }
            ]
            
            # ThÃªm lá»‹ch sá»­ chat (giá»›i háº¡n 10 tin nháº¯n gáº§n nháº¥t)
            recent_history = chat_history[-10:] if len(chat_history) > 10 else chat_history
            for msg in recent_history:
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            # ThÃªm tin nháº¯n hiá»‡n táº¡i
            messages.append({
                "role": "user",
                "content": user_input
            })
            
            # Gá»i Local LLM API
            response = self.client.chat.completions.create(
                model="local-model",
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            
            return response.choices[0].message.content
        
        # Sá»­ dá»¥ng safe_execute_with_retry Ä‘á»ƒ xá»­ lÃ½ lá»—i
        success, result, error_message = safe_execute_with_retry(
            _call_llm,
            max_retries=1,  # Chá»‰ thá»­ 1 láº§n vÃ¬ Local LLM
            delay=0.5,
            context="Local LLM Chat",
            show_user=False  # KhÃ´ng hiá»ƒn thá»‹ lá»—i raw
        )
        
        if success:
            return result
        else:
            # Xá»­ lÃ½ cÃ¡c loáº¡i lá»—i cá»¥ thá»ƒ
            return self._handle_llm_error(error_message, user_input, document_context)
    
    def _create_system_prompt(self, document_context):
        """Táº¡o system prompt cho chat thÃ´ng thÆ°á»ng"""
        base_prompt = """Báº¡n lÃ  má»™t AI assistant thÃ´ng minh vÃ  há»¯u Ã­ch. HÃ£y tráº£ lá»i cÃ¡c cÃ¢u há»i má»™t cÃ¡ch chi tiáº¿t vÃ  chÃ­nh xÃ¡c. Sá»­ dá»¥ng tiáº¿ng Viá»‡t Ä‘á»ƒ tráº£ lá»i."""
        
        if document_context:
            base_prompt += f"""
            
THÃ”NG TIN TÃ€I LIá»†U:
NgÆ°á»i dÃ¹ng Ä‘Ã£ upload cÃ¡c tÃ i liá»‡u sau. HÃ£y sá»­ dá»¥ng thÃ´ng tin nÃ y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i khi phÃ¹ há»£p:

{document_context[:3000]}...

Khi tráº£ lá»i dá»±a trÃªn tÃ i liá»‡u, hÃ£y ghi rÃµ báº¡n Ä‘ang tham kháº£o tá»« tÃ i liá»‡u Ä‘Ã£ upload.
"""
        
        return base_prompt
    
    def _create_document_qa_prompt(self, document_context):
        """Táº¡o system prompt chuyÃªn biá»‡t cho document Q&A"""
        return f"""Báº¡n lÃ  má»™t AI assistant chuyÃªn tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p.

NHIá»†M Vá»¤:
- Tráº£ lá»i cÃ¢u há»i chÃ­nh xÃ¡c vÃ  chi tiáº¿t dá»±a trÃªn ná»™i dung tÃ i liá»‡u
- Chá»‰ sá»­ dá»¥ng thÃ´ng tin cÃ³ trong tÃ i liá»‡u
- Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, hÃ£y nÃ³i rÃµ
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, sÃºc tÃ­ch vÃ  dá»… hiá»ƒu

NGUYÃŠN Táº®C:
âœ… Dá»±a hoÃ n toÃ n vÃ o ná»™i dung tÃ i liá»‡u
âœ… TrÃ­ch dáº«n cá»¥ thá»ƒ khi cáº§n thiáº¿t  
âœ… Thá»«a nháº­n khi khÃ´ng cÃ³ thÃ´ng tin
âŒ KhÃ´ng bá»‹a Ä‘áº·t thÃ´ng tin khÃ´ng cÃ³ trong tÃ i liá»‡u
âŒ KhÃ´ng suy diá»…n quÃ¡ xa

TÃ€I LIá»†U THAM KHáº¢O:
{document_context[:2500]}

HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u trÃªn."""
    
    def _generate_demo_response(self, user_input, document_context):
        """Táº¡o pháº£n há»“i demo khi khÃ´ng cÃ³ Local LLM connection"""
        if document_context:
            return f"""ğŸ¤– **Pháº£n há»“i Demo vá»›i TÃ i liá»‡u**

CÃ¢u há»i cá»§a báº¡n: "{user_input}"

Dá»±a trÃªn tÃ i liá»‡u báº¡n Ä‘Ã£ upload, tÃ´i cÃ³ thá»ƒ tháº¥y:
- TÃ i liá»‡u chá»©a {len(document_context)} kÃ½ tá»±
- Ná»™i dung liÃªn quan Ä‘áº¿n cÃ¢u há»i cá»§a báº¡n

**LÆ°u Ã½:** ÄÃ¢y lÃ  cháº¿ Ä‘á»™ demo. Äá»ƒ cÃ³ tráº£i nghiá»‡m Ä‘áº§y Ä‘á»§ vá»›i Local LLM:

**CÃ¡ch kÃ­ch hoáº¡t:**
1. ğŸš€ Khá»Ÿi Ä‘á»™ng LM Studio trÃªn port 1234
2. ğŸ“ Load má»™t model báº¥t ká»³ (llama, mistral, etc.)
3. â–¶ï¸ Restart á»©ng dá»¥ng Streamlit

Preview ná»™i dung tÃ i liá»‡u: {document_context[:200]}..."""
        
        return f"""ğŸ¤– **Pháº£n há»“i Demo - Local LLM**

CÃ¢u há»i cá»§a báº¡n: "{user_input}"

Xin chÃ o! TÃ´i lÃ  Study Buddy vá»›i Local LLM support. Hiá»‡n táº¡i Ä‘ang cháº¡y á»Ÿ cháº¿ Ä‘á»™ demo.

**ğŸ¯ Äá»ƒ demo hoáº¡t Ä‘á»™ng Ä‘áº§y Ä‘á»§:**

**BÆ°á»›c 1:** Khá»Ÿi Ä‘á»™ng LM Studio
- Download LM Studio tá»«: https://lmstudio.ai
- Má»Ÿ LM Studio vÃ  start server trÃªn port 1234

**BÆ°á»›c 2:** Load Model
- Download má»™t model (llama, mistral, phi, v.v.)
- Load model vÃ  start local server

**BÆ°á»›c 3:** Restart App
- Restart á»©ng dá»¥ng Streamlit nÃ y
- Báº¡n sáº½ tháº¥y "âœ… ÄÃ£ káº¿t ná»‘i Local LLM"

**ğŸ’¡ Lá»£i Ã­ch Local LLM:**
- âœ… HoÃ n toÃ n miá»…n phÃ­
- âœ… Cháº¡y offline 
- âœ… Privacy tuyá»‡t Ä‘á»‘i
- âœ… Demo á»•n Ä‘á»‹nh

Thá»i gian: {datetime.now().strftime('%H:%M:%S %d/%m/%Y')}"""
    
    def _handle_llm_error(self, error_message, user_input, document_context):
        """Xá»­ lÃ½ cÃ¡c loáº¡i lá»—i LLM cá»¥ thá»ƒ vÃ  tráº£ vá» pháº£n há»“i thÃ¢n thiá»‡n"""
        
        # Xá»­ lÃ½ lá»—i 404 - No models loaded
        if "404" in str(error_message) and "model_not_found" in str(error_message):
            return f"""âš ï¸ **Local LLM chÆ°a sáºµn sÃ ng**

CÃ¢u há»i cá»§a báº¡n: "{user_input}"

**ğŸ”§ NguyÃªn nhÃ¢n:** LM Studio chÆ°a load model nÃ o.

**ğŸ’¡ CÃ¡ch kháº¯c phá»¥c:**
1. ğŸš€ Má»Ÿ LM Studio
2. ğŸ“ Táº£i vÃ  load má»™t model (vÃ­ dá»¥: Llama, Mistral, Phi)
3. â–¶ï¸ Start local server trÃªn port 1234
4. ğŸ”„ Refresh láº¡i trang nÃ y

**ğŸ“‹ HÆ°á»›ng dáº«n chi tiáº¿t:**
- Download LM Studio: https://lmstudio.ai
- Chá»n tab "Chat" â†’ Load model â†’ Start server
- Äáº£m báº£o server cháº¡y trÃªn `http://localhost:1234`

**ğŸ¯ Tráº¡ng thÃ¡i hiá»‡n táº¡i:** Cháº¿ Ä‘á»™ chá» Local LLM
â° {datetime.now().strftime('%H:%M:%S %d/%m/%Y')}"""

        # Xá»­ lÃ½ lá»—i connection
        elif "connection" in str(error_message).lower() or "connect" in str(error_message).lower():
            return f"""ğŸ”Œ **Lá»—i káº¿t ná»‘i Local LLM**

CÃ¢u há»i cá»§a báº¡n: "{user_input}"

**ğŸ”§ NguyÃªn nhÃ¢n:** KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘áº¿n LM Studio server.

**ğŸ’¡ CÃ¡ch kháº¯c phá»¥c:**
1. âœ… Kiá»ƒm tra LM Studio Ä‘Ã£ cháº¡y chÆ°a
2. ğŸŒ Äáº£m báº£o server trÃªn port 1234
3. ğŸ”’ Táº¯t firewall/antivirus táº¡m thá»i
4. ğŸ”„ Restart LM Studio vÃ  á»©ng dá»¥ng nÃ y

**ğŸ“Š ThÃ´ng tin ká»¹ thuáº­t:**
- URL: {self.local_llm_url}
- Tráº¡ng thÃ¡i: Máº¥t káº¿t ná»‘i

â° {datetime.now().strftime('%H:%M:%S %d/%m/%Y')}"""

        # Xá»­ lÃ½ lá»—i timeout
        elif "timeout" in str(error_message).lower():
            return f"""â±ï¸ **Local LLM pháº£n há»“i cháº­m**

CÃ¢u há»i cá»§a báº¡n: "{user_input}"

**ğŸ”§ NguyÃªn nhÃ¢n:** Model xá»­ lÃ½ quÃ¡ lÃ¢u hoáº·c server quÃ¡ táº£i.

**ğŸ’¡ Gá»£i Ã½:**
1. ğŸ”„ Thá»­ láº¡i vá»›i cÃ¢u há»i ngáº¯n hÆ¡n
2. âš¡ Chá»n model nhá» hÆ¡n trong LM Studio
3. ğŸ–¥ï¸ ÄÃ³ng cÃ¡c á»©ng dá»¥ng khÃ¡c Ä‘á»ƒ giáº£i phÃ³ng RAM
4. â° Chá» má»™t chÃºt rá»“i thá»­ láº¡i

**ğŸ“Š Model hiá»‡n táº¡i cÃ³ thá»ƒ quÃ¡ lá»›n cho mÃ¡y nÃ y**

â° {datetime.now().strftime('%H:%M:%S %d/%m/%Y')}"""
        
        # Lá»—i general fallback
        else:
            return f"""ğŸ¤– **Táº¡m thá»i khÃ´ng thá»ƒ pháº£n há»“i**

CÃ¢u há»i cá»§a báº¡n: "{user_input}"

**ğŸ”§ Äang gáº·p sá»± cá»‘ ká»¹ thuáº­t vá»›i Local LLM**

**ğŸ’¡ CÃ³ thá»ƒ thá»­:**
1. ğŸ”„ Thá»­ láº¡i sau vÃ i giÃ¢y
2. ğŸš€ Restart LM Studio
3. ğŸ“± Kiá»ƒm tra model Ä‘Ã£ load Ä‘Ãºng chÆ°a
4. ğŸ’» Restart á»©ng dá»¥ng nÃ y

**ğŸ¯ Cháº¿ Ä‘á»™ dá»± phÃ²ng:** Demo mode
- á»¨ng dá»¥ng váº«n hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng
- Local LLM sáº½ tá»± phá»¥c há»“i khi sáºµn sÃ ng

â° {datetime.now().strftime('%H:%M:%S %d/%m/%Y')}
ğŸ”§ Lá»—i: {str(error_message)[:100]}..."""
