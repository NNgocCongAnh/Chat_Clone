import streamlit as st
import openai
from datetime import datetime
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class ChatHandler:
    def __init__(self):
        # Khá»Ÿi táº¡o Local LLM client (LM Studio)
        self.local_llm_url = os.getenv("LOCAL_LLM_URL", "http://localhost:1234/v1")
        try:
            self.client = openai.OpenAI(
                base_url=self.local_llm_url,
                api_key="not_needed"
            )
            self.api_key = "local_connected"  # Äá»ƒ tÆ°Æ¡ng thÃ­ch vá»›i logic cÅ©
            st.success("âœ… ChatHandler Ä‘Ã£ káº¿t ná»‘i Local LLM (LM Studio)")
        except Exception as e:
            self.client = None
            self.api_key = None
            st.warning(f"âš ï¸ ChatHandler khÃ´ng thá»ƒ káº¿t ná»‘i Local LLM: {str(e)}. á»¨ng dá»¥ng sáº½ cháº¡y á»Ÿ cháº¿ Ä‘á»™ demo.")
    
    def generate_response(self, user_input, chat_history, document_context="", is_document_qa=False):
        """Táº¡o pháº£n há»“i tá»« AI vá»›i enhanced document Q&A support"""
        
        # Náº¿u khÃ´ng cÃ³ API key, tráº£ vá» pháº£n há»“i demo
        if not self.api_key:
            return self._generate_demo_response(user_input, document_context)
        
        try:
            # Chuáº©n bá»‹ messages cho OpenAI
            if is_document_qa and document_context:
                # Specialized system prompt cho document Q&A
                system_content = self._create_document_qa_prompt(document_context)
                max_tokens = 500  # Focused answers
                temperature = 0.2  # More deterministic
            else:
                # Standard chat system prompt
                system_content = self._create_system_prompt(document_context)
                max_tokens = 1000
                temperature = 0.7
            
            messages = [
                {
                    "role": "system",
                    "content": system_content
                }
            ]
            
            # ThÃªm lá»‹ch sá»­ chat (giá»›i háº¡n 10 tin nháº¯n gáº§n nháº¥t)
            recent_history = chat_history[-10:] if len(chat_history) > 10 else chat_history
            for msg in recent_history:
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            # ThÃªm tin nháº¯n hiá»‡n táº¡i
            messages.append({
                "role": "user",
                "content": user_input
            })
            
            # Gá»i Local LLM API
            response = self.client.chat.completions.create(
                model="local-model",
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            st.error(f"Lá»—i khi gá»i OpenAI API: {str(e)}")
            return "Xin lá»—i, tÃ´i Ä‘ang gáº·p sá»± cá»‘ ká»¹ thuáº­t. Vui lÃ²ng thá»­ láº¡i sau."
    
    def _create_system_prompt(self, document_context):
        """Táº¡o system prompt cho chat thÃ´ng thÆ°á»ng"""
        base_prompt = """Báº¡n lÃ  má»™t AI assistant thÃ´ng minh vÃ  há»¯u Ã­ch. HÃ£y tráº£ lá»i cÃ¡c cÃ¢u há»i má»™t cÃ¡ch chi tiáº¿t vÃ  chÃ­nh xÃ¡c. Sá»­ dá»¥ng tiáº¿ng Viá»‡t Ä‘á»ƒ tráº£ lá»i."""
        
        if document_context:
            base_prompt += f"""
            
THÃ”NG TIN TÃ€I LIá»†U:
NgÆ°á»i dÃ¹ng Ä‘Ã£ upload cÃ¡c tÃ i liá»‡u sau. HÃ£y sá»­ dá»¥ng thÃ´ng tin nÃ y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i khi phÃ¹ há»£p:

{document_context[:3000]}...

Khi tráº£ lá»i dá»±a trÃªn tÃ i liá»‡u, hÃ£y ghi rÃµ báº¡n Ä‘ang tham kháº£o tá»« tÃ i liá»‡u Ä‘Ã£ upload.
"""
        
        return base_prompt
    
    def _create_document_qa_prompt(self, document_context):
        """Táº¡o system prompt chuyÃªn biá»‡t cho document Q&A"""
        return f"""Báº¡n lÃ  má»™t AI assistant chuyÃªn tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p.

NHIá»†M Vá»¤:
- Tráº£ lá»i cÃ¢u há»i chÃ­nh xÃ¡c vÃ  chi tiáº¿t dá»±a trÃªn ná»™i dung tÃ i liá»‡u
- Chá»‰ sá»­ dá»¥ng thÃ´ng tin cÃ³ trong tÃ i liá»‡u
- Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, hÃ£y nÃ³i rÃµ
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, sÃºc tÃ­ch vÃ  dá»… hiá»ƒu

NGUYÃŠN Táº®C:
âœ… Dá»±a hoÃ n toÃ n vÃ o ná»™i dung tÃ i liá»‡u
âœ… TrÃ­ch dáº«n cá»¥ thá»ƒ khi cáº§n thiáº¿t  
âœ… Thá»«a nháº­n khi khÃ´ng cÃ³ thÃ´ng tin
âŒ KhÃ´ng bá»‹a Ä‘áº·t thÃ´ng tin khÃ´ng cÃ³ trong tÃ i liá»‡u
âŒ KhÃ´ng suy diá»…n quÃ¡ xa

TÃ€I LIá»†U THAM KHáº¢O:
{document_context[:2500]}

HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u trÃªn."""
    
    def _generate_demo_response(self, user_input, document_context):
        """Táº¡o pháº£n há»“i demo khi khÃ´ng cÃ³ Local LLM connection"""
        if document_context:
            return f"""ğŸ¤– **Pháº£n há»“i Demo vá»›i TÃ i liá»‡u**

CÃ¢u há»i cá»§a báº¡n: "{user_input}"

Dá»±a trÃªn tÃ i liá»‡u báº¡n Ä‘Ã£ upload, tÃ´i cÃ³ thá»ƒ tháº¥y:
- TÃ i liá»‡u chá»©a {len(document_context)} kÃ½ tá»±
- Ná»™i dung liÃªn quan Ä‘áº¿n cÃ¢u há»i cá»§a báº¡n

**LÆ°u Ã½:** ÄÃ¢y lÃ  cháº¿ Ä‘á»™ demo. Äá»ƒ cÃ³ tráº£i nghiá»‡m Ä‘áº§y Ä‘á»§ vá»›i Local LLM:

**CÃ¡ch kÃ­ch hoáº¡t:**
1. ğŸš€ Khá»Ÿi Ä‘á»™ng LM Studio trÃªn port 1234
2. ğŸ“ Load má»™t model báº¥t ká»³ (llama, mistral, etc.)
3. â–¶ï¸ Restart á»©ng dá»¥ng Streamlit

Preview ná»™i dung tÃ i liá»‡u: {document_context[:200]}..."""
        
        return f"""ğŸ¤– **Pháº£n há»“i Demo - Local LLM**

CÃ¢u há»i cá»§a báº¡n: "{user_input}"

Xin chÃ o! TÃ´i lÃ  Study Buddy vá»›i Local LLM support. Hiá»‡n táº¡i Ä‘ang cháº¡y á»Ÿ cháº¿ Ä‘á»™ demo.

**ğŸ¯ Äá»ƒ demo hoáº¡t Ä‘á»™ng Ä‘áº§y Ä‘á»§:**

**BÆ°á»›c 1:** Khá»Ÿi Ä‘á»™ng LM Studio
- Download LM Studio tá»«: https://lmstudio.ai
- Má»Ÿ LM Studio vÃ  start server trÃªn port 1234

**BÆ°á»›c 2:** Load Model
- Download má»™t model (llama, mistral, phi, v.v.)
- Load model vÃ  start local server

**BÆ°á»›c 3:** Restart App
- Restart á»©ng dá»¥ng Streamlit nÃ y
- Báº¡n sáº½ tháº¥y "âœ… ÄÃ£ káº¿t ná»‘i Local LLM"

**ğŸ’¡ Lá»£i Ã­ch Local LLM:**
- âœ… HoÃ n toÃ n miá»…n phÃ­
- âœ… Cháº¡y offline 
- âœ… Privacy tuyá»‡t Ä‘á»‘i
- âœ… Demo á»•n Ä‘á»‹nh

Thá»i gian: {datetime.now().strftime('%H:%M:%S %d/%m/%Y')}"""
