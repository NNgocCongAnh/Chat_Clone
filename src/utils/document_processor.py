import docx
import streamlit as st
from io import BytesIO
import base64
from mistralai import Mistral
import openai
import os
from typing import Dict, Optional, List, Tuple
import fitz  # PyMuPDF
from PIL import Image
from .validators import FileValidator, DocumentValidator
from .error_handler import (
    handle_error, error_boundary, FileProcessingError, 
    LLMConnectionError, show_warning_message, ProgressTracker,
    with_progress, safe_execute_with_retry
)
from ..config.constants import (
    MAX_DOCUMENT_CHARS, DEFAULT_PDF_DPI, MAX_PDF_DPI, MIN_PDF_DPI,
    WARNING_MESSAGES, ERROR_MESSAGES
)

class DocumentProcessor:
    def __init__(self):
        # Kh·ªüi t·∫°o Mistral client
        self.mistral_api_key = os.getenv("MISTRAL_API_KEY")
        if not self.mistral_api_key:
            try:
                self.mistral_api_key = st.secrets.get("MISTRAL_API_KEY", "")
            except Exception:
                self.mistral_api_key = ""
        
        if self.mistral_api_key:
            self.mistral_client = Mistral(api_key=self.mistral_api_key)
        else:
            self.mistral_client = None
            st.warning("‚ö†Ô∏è Ch∆∞a c·∫•u h√¨nh Mistral API Key. Ch·ª©c nƒÉng OCR PDF s·∫Ω b·ªã h·∫°n ch·∫ø.")
        
        # Kh·ªüi t·∫°o Local LLM client (LM Studio)
        self.local_llm_url = os.getenv("LOCAL_LLM_URL", "http://localhost:1234/v1")
        try:
            self.openai_client = openai.OpenAI(
                base_url=self.local_llm_url,
                api_key="not_needed"
            )
            st.success("‚úÖ ƒê√£ k·∫øt n·ªëi Local LLM (LM Studio)")
        except Exception as e:
            self.openai_client = None
            st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ k·∫øt n·ªëi Local LLM: {str(e)}. ƒê·∫£m b·∫£o LM Studio ƒëang ch·∫°y tr√™n {self.local_llm_url}")
        
        self.embeddings_cache = {}
    
    @error_boundary("Document processing", show_user=True)
    def process_document(self, uploaded_file) -> Optional[str]:
        """
        X·ª≠ l√Ω v√† tr√≠ch xu·∫•t text t·ª´ t√†i li·ªáu ƒë√£ upload v·ªõi validation v√† error handling
        
        Args:
            uploaded_file: Streamlit UploadedFile object
            
        Returns:
            Extracted text content ho·∫∑c None n·∫øu l·ªói
        """
        # Validate file tr∆∞·ªõc khi x·ª≠ l√Ω
        validation_result = FileValidator.validate_file(uploaded_file)
        if not validation_result.is_valid:
            raise FileProcessingError(validation_result.error_message, "file_validation_failed")
        
        # Check file size warning
        if uploaded_file.size > 50 * 1024 * 1024:  # 50MB
            show_warning_message('large_file')
        
        file_extension = FileValidator.get_file_extension(uploaded_file.name)
        if not file_extension:
            raise FileProcessingError("Kh√¥ng th·ªÉ x√°c ƒë·ªãnh ƒë·ªãnh d·∫°ng file", "unknown_format")
        
        # Progress tracking cho c√°c lo·∫°i file kh√°c nhau
        if file_extension == 'pdf':
            return self._process_pdf_with_progress(uploaded_file)
        elif file_extension == 'docx':
            return self._process_docx_safe(uploaded_file)
        elif file_extension in ['txt', 'md']:
            return self._process_text_safe(uploaded_file)
        else:
            raise FileProcessingError(
                f"ƒê·ªãnh d·∫°ng file {file_extension} kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£", 
                "unsupported_format"
            )
    
    @with_progress(3, "X·ª≠ l√Ω file PDF")
    def _process_pdf_with_progress(self, uploaded_file, progress_tracker=None) -> Optional[str]:
        """X·ª≠ l√Ω PDF v·ªõi progress tracking"""
        if progress_tracker:
            progress_tracker.update("Ki·ªÉm tra file PDF")
        
        # Check PDF page count first
        page_count = self.get_pdf_page_count_with_pymupdf(uploaded_file)
        if page_count > 50:
            show_warning_message('many_pages')
        
        if progress_tracker:
            progress_tracker.update("Th·ª±c hi·ªán OCR")
        
        content = self.extract_pdf_content(uploaded_file)
        
        if progress_tracker:
            progress_tracker.update("Ki·ªÉm tra k·∫øt qu·∫£")
        
        if content:
            # Validate content length
            is_valid_content, content_error = DocumentValidator.validate_document_content(content)
            if not is_valid_content:
                raise FileProcessingError(content_error, "content_validation_failed")
        
        return content
    
    @error_boundary("DOCX processing", show_user=True)
    def _process_docx_safe(self, uploaded_file) -> Optional[str]:
        """X·ª≠ l√Ω DOCX v·ªõi error handling"""
        try:
            with st.spinner("ƒêang x·ª≠ l√Ω file DOCX..."):
                content = self.extract_docx_content(uploaded_file)
                
                if content:
                    # Validate content
                    is_valid, error_msg = DocumentValidator.validate_document_content(content)
                    if not is_valid:
                        raise FileProcessingError(error_msg, "content_validation_failed")
                
                return content
        except Exception as e:
            raise FileProcessingError(f"L·ªói x·ª≠ l√Ω DOCX: {str(e)}", "docx_processing_failed")
    
    @error_boundary("Text processing", show_user=True)
    def _process_text_safe(self, uploaded_file) -> Optional[str]:
        """X·ª≠ l√Ω text file v·ªõi error handling"""
        try:
            with st.spinner("ƒêang ƒë·ªçc file text..."):
                content = self.extract_text_content(uploaded_file)
                
                if content:
                    # Validate content
                    is_valid, error_msg = DocumentValidator.validate_document_content(content)
                    if not is_valid:
                        raise FileProcessingError(error_msg, "content_validation_failed")
                
                return content
        except Exception as e:
            raise FileProcessingError(f"L·ªói x·ª≠ l√Ω text file: {str(e)}", "text_processing_failed")
    
    def extract_pdf_content(self, uploaded_file):
        """Tr√≠ch xu·∫•t text t·ª´ file PDF s·ª≠ d·ª•ng Mistral OCR"""
        if not self.mistral_client:
            st.error("Mistral API Key ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh. Kh√¥ng th·ªÉ x·ª≠ l√Ω PDF.")
            return None
            
        try:
            # M√£ h√≥a PDF th√†nh base64
            uploaded_file.seek(0)
            pdf_bytes = uploaded_file.read()
            base64_pdf = base64.b64encode(pdf_bytes).decode('utf-8')
            
            # G·ªçi Mistral OCR API
            with st.spinner("ƒêang x·ª≠ l√Ω PDF b·∫±ng Mistral OCR..."):
                ocr_response = self.mistral_client.ocr.process(
                    model="mistral-ocr-latest",
                    document={
                        "type": "document_url",
                        "document_url": f"data:application/pdf;base64,{base64_pdf}"
                    }
                )
                
                # L∆∞u th√¥ng tin t·ª´ng trang v√†o cache
                self.pdf_pages_cache = {}
                
                # Tr√≠ch xu·∫•t n·ªôi dung t·ª´ t·∫•t c·∫£ c√°c trang
                content = ""
                for i, page in enumerate(ocr_response.pages):
                    if hasattr(page, 'markdown') and page.markdown:
                        page_content = page.markdown.strip()
                        content += page_content + "\n\n"
                        # Cache n·ªôi dung t·ª´ng trang
                        self.pdf_pages_cache[i + 1] = page_content
                
                return content.strip() if content else None
                
        except Exception as e:
            st.error(f"L·ªói khi x·ª≠ l√Ω PDF v·ªõi Mistral OCR: {str(e)}")
            return None
    
    def get_pdf_page_count(self, uploaded_file) -> int:
        """
        L·∫•y s·ªë trang c·ªßa file PDF
        
        Args:
            uploaded_file: File PDF ƒë√£ upload
            
        Returns:
            S·ªë trang c·ªßa PDF
        """
        if not self.mistral_client:
            st.error("Mistral API Key ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh.")
            return 0
            
        try:
            # N·∫øu ƒë√£ c√≥ cache t·ª´ l·∫ßn x·ª≠ l√Ω tr∆∞·ªõc
            if hasattr(self, 'pdf_pages_cache') and self.pdf_pages_cache:
                return len(self.pdf_pages_cache)
            
            # M√£ h√≥a PDF th√†nh base64
            uploaded_file.seek(0)
            pdf_bytes = uploaded_file.read()
            base64_pdf = base64.b64encode(pdf_bytes).decode('utf-8')
            
            # G·ªçi Mistral OCR API ƒë·ªÉ l·∫•y th√¥ng tin trang
            ocr_response = self.mistral_client.ocr.process(
                model="mistral-ocr-latest",
                document={
                    "type": "document_url",
                    "document_url": f"data:application/pdf;base64,{base64_pdf}"
                }
            )
            
            return len(ocr_response.pages) if ocr_response.pages else 0
            
        except Exception as e:
            st.error(f"L·ªói khi ƒë·∫øm trang PDF: {str(e)}")
            return 0
    
    def extract_specific_pdf_page(self, uploaded_file, page_number: int) -> Optional[str]:
        """
        Tr√≠ch xu·∫•t n·ªôi dung t·ª´ m·ªôt trang c·ª• th·ªÉ c·ªßa PDF
        
        Args:
            uploaded_file: File PDF ƒë√£ upload
            page_number: S·ªë trang c·∫ßn tr√≠ch xu·∫•t (b·∫Øt ƒë·∫ßu t·ª´ 1)
            
        Returns:
            N·ªôi dung c·ªßa trang ƒë√≥ ho·∫∑c None n·∫øu l·ªói
        """
        if not self.mistral_client:
            st.error("Mistral API Key ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh.")
            return None
            
        try:
            # Ki·ªÉm tra cache tr∆∞·ªõc
            if hasattr(self, 'pdf_pages_cache') and page_number in self.pdf_pages_cache:
                return self.pdf_pages_cache[page_number]
            
            # M√£ h√≥a PDF th√†nh base64
            uploaded_file.seek(0)
            pdf_bytes = uploaded_file.read()
            base64_pdf = base64.b64encode(pdf_bytes).decode('utf-8')
            
            # G·ªçi Mistral OCR API
            with st.spinner(f"ƒêang x·ª≠ l√Ω trang {page_number}..."):
                ocr_response = self.mistral_client.ocr.process(
                    model="mistral-ocr-latest",
                    document={
                        "type": "document_url",
                        "document_url": f"data:application/pdf;base64,{base64_pdf}"
                    }
                )
                
                # L·∫•y n·ªôi dung trang c·ª• th·ªÉ
                if ocr_response.pages and len(ocr_response.pages) >= page_number:
                    page = ocr_response.pages[page_number - 1]  # Index t·ª´ 0
                    if hasattr(page, 'markdown') and page.markdown:
                        return page.markdown.strip()
                
                return None
                
        except Exception as e:
            st.error(f"L·ªói khi x·ª≠ l√Ω trang {page_number}: {str(e)}")
            return None
    
    def extract_pdf_pages_range(self, uploaded_file, start_page: int, end_page: int) -> Dict[int, str]:
        """
        Tr√≠ch xu·∫•t n·ªôi dung t·ª´ m·ªôt d·∫£i trang c·ªßa PDF
        
        Args:
            uploaded_file: File PDF ƒë√£ upload
            start_page: Trang b·∫Øt ƒë·∫ßu (t·ª´ 1)
            end_page: Trang k·∫øt th√∫c (bao g·ªìm)
            
        Returns:
            Dict v·ªõi key l√† s·ªë trang, value l√† n·ªôi dung
        """
        if not self.mistral_client:
            st.error("Mistral API Key ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh.")
            return {}
            
        try:
            # M√£ h√≥a PDF th√†nh base64
            uploaded_file.seek(0)
            pdf_bytes = uploaded_file.read()
            base64_pdf = base64.b64encode(pdf_bytes).decode('utf-8')
            
            # G·ªçi Mistral OCR API
            with st.spinner(f"ƒêang x·ª≠ l√Ω trang {start_page}-{end_page}..."):
                ocr_response = self.mistral_client.ocr.process(
                    model="mistral-ocr-latest",
                    document={
                        "type": "document_url",
                        "document_url": f"data:application/pdf;base64,{base64_pdf}"
                    }
                )
                
                # Tr√≠ch xu·∫•t n·ªôi dung t·ª´ d·∫£i trang
                pages_content = {}
                if ocr_response.pages:
                    for page_num in range(start_page, min(end_page + 1, len(ocr_response.pages) + 1)):
                        page_index = page_num - 1  # Index t·ª´ 0
                        if page_index < len(ocr_response.pages):
                            page = ocr_response.pages[page_index]
                            if hasattr(page, 'markdown') and page.markdown:
                                pages_content[page_num] = page.markdown.strip()
                
                return pages_content
                
        except Exception as e:
            st.error(f"L·ªói khi x·ª≠ l√Ω trang {start_page}-{end_page}: {str(e)}")
            return {}
    
    def extract_docx_content(self, uploaded_file):
        """Tr√≠ch xu·∫•t text t·ª´ file DOCX"""
        try:
            doc = docx.Document(BytesIO(uploaded_file.read()))
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text.strip()
        except Exception as e:
            st.error(f"Kh√¥ng th·ªÉ ƒë·ªçc file DOCX: {str(e)}")
            return None
    
    def extract_text_content(self, uploaded_file):
        """Tr√≠ch xu·∫•t text t·ª´ file TXT/MD"""
        try:
            # Th·ª≠ nhi·ªÅu encoding
            encodings = ['utf-8', 'utf-16', 'latin-1', 'cp1252']
            
            for encoding in encodings:
                try:
                    uploaded_file.seek(0)
                    content = uploaded_file.read().decode(encoding)
                    return content
                except UnicodeDecodeError:
                    continue
            
            # N·∫øu t·∫•t c·∫£ encoding ƒë·ªÅu th·∫•t b·∫°i
            st.error("Kh√¥ng th·ªÉ ƒë·ªçc file text v·ªõi encoding ph√π h·ª£p")
            return None
            
        except Exception as e:
            st.error(f"Kh√¥ng th·ªÉ ƒë·ªçc file text: {str(e)}")
            return None
    
    def chunk_text(self, text, chunk_size=1000, overlap=200):
        """Chia text th√†nh c√°c chunks nh·ªè h∆°n ƒë·ªÉ x·ª≠ l√Ω"""
        if not text:
            return []
        
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + chunk_size
            
            # T√¨m ƒëi·ªÉm ng·∫Øt t·ª± nhi√™n (k·∫øt th√∫c c√¢u ho·∫∑c ƒëo·∫°n)
            if end < text_length:
                # T√¨m ng·∫Øt c√¢u g·∫ßn nh·∫•t
                for i in range(end, start + chunk_size - 100, -1):
                    if text[i] in '.!?\n':
                        end = i + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap if end > overlap else end
        
        return chunks
    
    def summarize_text_with_openai(self, text, max_words=150):
        """T√≥m t·∫Øt vƒÉn b·∫£n s·ª≠ d·ª•ng Local LLM"""
        if not self.openai_client:
            return "Kh√¥ng th·ªÉ k·∫øt n·ªëi Local LLM. Vui l√≤ng kh·ªüi ƒë·ªông LM Studio tr∆∞·ªõc."
            
        try:
            # Chia text th√†nh chunks n·∫øu qu√° d√†i (GPT c√≥ gi·ªõi h·∫°n token)
            max_input_length = 3000  # Kho·∫£ng 3000 characters ~ 750 tokens
            
            if len(text) > max_input_length:
                chunks = self.chunk_text(text, chunk_size=max_input_length, overlap=200)
                summaries = []
                
                # T√≥m t·∫Øt t·ª´ng chunk
                for i, chunk in enumerate(chunks[:4]):  # Gi·ªõi h·∫°n 4 chunks ƒë·ªÉ tr√°nh cost qu√° cao
                    if len(chunk.strip()) > 100:
                        chunk_summary = self._summarize_chunk_with_openai(chunk, max_words//len(chunks))
                        if chunk_summary:
                            summaries.append(chunk_summary)
                
                # K·∫øt h·ª£p v√† t√≥m t·∫Øt final
                combined_text = " ".join(summaries)
                if len(combined_text) > max_input_length:
                    final_summary = self._summarize_chunk_with_openai(combined_text, max_words)
                    return final_summary
                else:
                    return combined_text
            else:
                return self._summarize_chunk_with_openai(text, max_words)
                
        except Exception as e:
            st.error(f"L·ªói khi t√≥m t·∫Øt v·ªõi OpenAI: {str(e)}")
            return f"Kh√¥ng th·ªÉ t√≥m t·∫Øt t√†i li·ªáu. L·ªói: {str(e)}"
    
    def _summarize_chunk_with_openai(self, text, max_words=150):
        """T√≥m t·∫Øt m·ªôt chunk text b·∫±ng Local LLM v·ªõi improved error handling"""
        
        def _call_summarize_llm():
            prompt = f"""H√£y t√≥m t·∫Øt n·ªôi dung sau b·∫±ng ti·∫øng Vi·ªát, kho·∫£ng {max_words} t·ª´. T·∫≠p trung v√†o nh·ªØng √Ω ch√≠nh v√† th√¥ng tin quan tr·ªçng nh·∫•t:

{text}

T√≥m t·∫Øt ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu:"""

            response = self.openai_client.chat.completions.create(
                model="local-model",
                messages=[
                    {"role": "system", "content": "B·∫°n l√† m·ªôt chuy√™n gia t√≥m t·∫Øt vƒÉn b·∫£n. H√£y t√≥m t·∫Øt n·ªôi dung m·ªôt c√°ch s√∫c t√≠ch v√† ch√≠nh x√°c b·∫±ng ti·∫øng Vi·ªát."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=300,
                temperature=0.3,  # Th·∫•p ƒë·ªÉ c√≥ k·∫øt qu·∫£ ·ªïn ƒë·ªãnh
            )
            
            return response.choices[0].message.content.strip()
        
        # S·ª≠ d·ª•ng safe_execute_with_retry
        success, result, error_message = safe_execute_with_retry(
            _call_summarize_llm,
            max_retries=1,
            delay=0.5,
            context="Local LLM Summarization",
            show_user=False
        )
        
        if success:
            return result
        else:
            # Fallback summary
            return f"üìÑ **T√≥m t·∫Øt t·ª± ƒë·ªông:** T√†i li·ªáu ch·ª©a {len(text)} k√Ω t·ª±. N·ªôi dung bao g·ªìm c√°c th√¥ng tin quan tr·ªçng c·∫ßn ƒë∆∞·ª£c ph√¢n t√≠ch chi ti·∫øt. (Local LLM kh√¥ng kh·∫£ d·ª•ng - {str(error_message)[:50]}...)"
    
    # ƒê·ªÉ t∆∞∆°ng th√≠ch backward, t·∫°o alias
    def summarize_text(self, text, max_length=200, min_length=50):
        """Alias cho backward compatibility"""
        max_words = max_length // 3  # Rough conversion
        return self.summarize_text_with_openai(text, max_words)
    
    def generate_questions(self, text):
        """T·∫°o c√¢u h·ªèi g·ª£i √Ω d·ª±a tr√™n n·ªôi dung t√†i li·ªáu v·ªõi improved error handling"""
        
        def _get_fallback_questions():
            """Tr·∫£ v·ªÅ c√¢u h·ªèi m·∫∑c ƒë·ªãnh"""
            return [
                "N·ªôi dung ch√≠nh c·ªßa t√†i li·ªáu l√† g√¨?",
                "C√°c ƒëi·ªÉm quan tr·ªçng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p?",
                "K·∫øt lu·∫≠n ch√≠nh t·ª´ t√†i li·ªáu n√†y?",
                "Th√¥ng tin n√†o ƒë√°ng ch√∫ √Ω nh·∫•t?"
            ]
        
        # N·∫øu kh√¥ng c√≥ Mistral client, tr·∫£ v·ªÅ c√¢u h·ªèi m·∫∑c ƒë·ªãnh
        if not self.mistral_client:
            return _get_fallback_questions()
        
        def _call_mistral_questions():
            # L·∫•y ƒëo·∫°n vƒÉn ƒë·∫°i di·ªán ƒë·ªÉ t·∫°o c√¢u h·ªèi
            chunks = self.chunk_text(text, chunk_size=500)
            sample_text = chunks[0] if chunks else text[:500]
            
            prompt = f"""D·ª±a tr√™n ƒëo·∫°n vƒÉn sau, t·∫°o 4 c√¢u h·ªèi ng·∫Øn g·ªçn m√† ng∆∞·ªùi ƒë·ªçc c√≥ th·ªÉ quan t√¢m:

{sample_text}

Y√™u c·∫ßu:
- M·ªói c√¢u h·ªèi kh√¥ng qu√° 15 t·ª´
- T·∫≠p trung v√†o th√¥ng tin ch√≠nh
- Ph√π h·ª£p v·ªõi n·ªôi dung vƒÉn b·∫£n
- Ch·ªâ tr·∫£ v·ªÅ 4 c√¢u h·ªèi, m·ªói c√¢u m·ªôt d√≤ng
"""
            
            response = self.mistral_client.chat.complete(
                model="mistral-large-latest",
                messages=[{"role": "user", "content": prompt}]
            )
            
            # T√°ch c√¢u h·ªèi th√†nh list
            questions_text = response.choices[0].message.content
            questions = [q.strip() for q in questions_text.split('\n') if q.strip() and '?' in q]
            
            return questions[:4] if questions else []
        
        # S·ª≠ d·ª•ng safe_execute_with_retry
        success, result, error_message = safe_execute_with_retry(
            _call_mistral_questions,
            max_retries=1,
            delay=0.5,
            context="Mistral Question Generation",
            show_user=False
        )
        
        if success and result and len(result) >= 3:
            return result
        else:
            # X·ª≠ l√Ω l·ªói c·ª• th·ªÉ v√† tr·∫£ v·ªÅ c√¢u h·ªèi ph√π h·ª£p
            return self._handle_mistral_question_error(error_message, text)
    
    def _handle_mistral_question_error(self, error_message, text):
        """X·ª≠ l√Ω l·ªói Mistral API khi t·∫°o c√¢u h·ªèi v√† t·∫°o c√¢u h·ªèi th√¥ng minh h∆°n"""
        
        # L·ªói 429 - Rate limit exceeded
        if "429" in str(error_message) or "capacity exceeded" in str(error_message).lower():
            # T·∫°o c√¢u h·ªèi th√¥ng minh d·ª±a tr√™n n·ªôi dung
            return self._generate_smart_questions_from_content(text)
        
        # L·ªói 401/403 - Authentication
        elif "401" in str(error_message) or "403" in str(error_message):
            return [
                "N·ªôi dung ch√≠nh c·ªßa t√†i li·ªáu l√† g√¨?",
                "C√°c th√¥ng tin quan tr·ªçng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p?",
                "ƒêi·ªÉm n·ªïi b·∫≠t trong t√†i li·ªáu?",
                "K·∫øt lu·∫≠n ch√≠nh t·ª´ n·ªôi dung n√†y?"
            ]
        
        # L·ªói k·∫øt n·ªëi
        elif "connection" in str(error_message).lower():
            return self._generate_smart_questions_from_content(text)
        
        # L·ªói kh√°c
        else:
            return self._generate_smart_questions_from_content(text)
    
    def _generate_smart_questions_from_content(self, text):
        """T·∫°o c√¢u h·ªèi th√¥ng minh d·ª±a tr√™n ph√¢n t√≠ch n·ªôi dung"""
        try:
            # L·∫•y m·ªôt ph·∫ßn nh·ªè c·ªßa text ƒë·ªÉ ph√¢n t√≠ch
            sample = text[:800] if len(text) > 800 else text
            sample_lower = sample.lower()
            
            questions = []
            
            # Ph√¢n t√≠ch keywords ƒë·ªÉ t·∫°o c√¢u h·ªèi ph√π h·ª£p
            if any(word in sample_lower for word in ['ƒë·ªãnh nghƒ©a', 'kh√°i ni·ªám', 'l√† g√¨']):
                questions.append("C√°c kh√°i ni·ªám ch√≠nh ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ th·∫ø n√†o?")
            
            if any(word in sample_lower for word in ['ph∆∞∆°ng ph√°p', 'c√°ch th·ª©c', 'quy tr√¨nh']):
                questions.append("Ph∆∞∆°ng ph√°p n√†o ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong t√†i li·ªáu?")
            
            if any(word in sample_lower for word in ['k·∫øt qu·∫£', 'th√†nh qu·∫£', 'hi·ªáu qu·∫£']):
                questions.append("K·∫øt qu·∫£ ch√≠nh ƒë·∫°t ƒë∆∞·ª£c l√† g√¨?")
            
            if any(word in sample_lower for word in ['v·∫•n ƒë·ªÅ', 'th√°ch th·ª©c', 'kh√≥ khƒÉn']):
                questions.append("Nh·ªØng v·∫•n ƒë·ªÅ n√†o ƒë∆∞·ª£c n√™u ra?")
            
            if any(word in sample_lower for word in ['gi·∫£i ph√°p', 'ƒë·ªÅ xu·∫•t', 'khuy·∫øn ngh·ªã']):
                questions.append("Gi·∫£i ph√°p n√†o ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t?")
            
            if any(word in sample_lower for word in ['ph√¢n t√≠ch', 'nghi√™n c·ª©u', 'kh·∫£o s√°t']):
                questions.append("Ph√¢n t√≠ch ch√≠nh trong t√†i li·ªáu l√† g√¨?")
            
            # ƒê·∫£m b·∫£o c√≥ √≠t nh·∫•t 4 c√¢u h·ªèi
            default_questions = [
                "N·ªôi dung ch√≠nh c·ªßa t√†i li·ªáu l√† g√¨?",
                "C√°c ƒëi·ªÉm quan tr·ªçng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p?",
                "Th√¥ng tin n√†o ƒë√°ng ch√∫ √Ω nh·∫•t?",
                "K·∫øt lu·∫≠n ch√≠nh t·ª´ t√†i li·ªáu n√†y?"
            ]
            
            # K·∫øt h·ª£p v√† lo·∫°i b·ªè tr√πng l·∫∑p
            all_questions = questions + default_questions
            unique_questions = []
            seen = set()
            
            for q in all_questions:
                if q not in seen:
                    unique_questions.append(q)
                    seen.add(q)
                if len(unique_questions) >= 4:
                    break
            
            return unique_questions[:4]
            
        except Exception as e:
            # Fallback cu·ªëi c√πng
            return [
                "N·ªôi dung ch√≠nh c·ªßa t√†i li·ªáu l√† g√¨?",
                "C√°c ƒëi·ªÉm quan tr·ªçng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p?",
                "Th√¥ng tin n√†o ƒë√°ng ch√∫ √Ω nh·∫•t?",
                "K·∫øt lu·∫≠n ch√≠nh t·ª´ t√†i li·ªáu n√†y?"
            ]
    
    def answer_question_with_openai(self, question, document_text):
        """Tr·∫£ l·ªùi c√¢u h·ªèi s·ª≠ d·ª•ng Local LLM v·ªõi RAG approach"""
        if not document_text:
            return "Kh√¥ng c√≥ t√†i li·ªáu n√†o ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi."
        
        if not self.openai_client:
            return "Kh√¥ng th·ªÉ k·∫øt n·ªëi Local LLM. Vui l√≤ng kh·ªüi ƒë·ªông LM Studio tr∆∞·ªõc."
            
        try:
            # T√¨m ph·∫ßn vƒÉn b·∫£n li√™n quan ƒë·∫øn c√¢u h·ªèi
            relevant_text = self._find_relevant_text_for_question(question, document_text, max_length=2500)
            
            # T·∫°o system prompt chuy√™n bi·ªát cho Q&A
            system_prompt = """B·∫°n l√† m·ªôt AI assistant chuy√™n tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p. 
H√£y tr·∫£ l·ªùi ch√≠nh x√°c, chi ti·∫øt v√† d·ª±a ho√†n to√†n v√†o n·ªôi dung t√†i li·ªáu. 
N·∫øu th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu, h√£y n√≥i r√µ l√† kh√¥ng t√¨m th·∫•y th√¥ng tin ƒë√≥."""
            
            # T·∫°o user prompt v·ªõi context v√† question
            user_prompt = f"""D·ª±a v√†o ƒëo·∫°n vƒÉn b·∫£n sau, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† chi ti·∫øt:

ƒêO·∫†N VƒÇN B·∫¢N:
{relevant_text}

C√ÇU H·ªéI: {question}

H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, d·ª±a ho√†n to√†n v√†o th√¥ng tin trong ƒëo·∫°n vƒÉn b·∫£n tr√™n:"""

            response = self.openai_client.chat.completions.create(
                model="local-model",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=400,
                temperature=0.2,  # Th·∫•p ƒë·ªÉ c√≥ c√¢u tr·∫£ l·ªùi ch√≠nh x√°c
            )
            
            answer = response.choices[0].message.content.strip()
            
            # Th√™m prefix ƒë·ªÉ ng∆∞·ªùi d√πng bi·∫øt ƒë√¢y l√† c√¢u tr·∫£ l·ªùi d·ª±a tr√™n t√†i li·ªáu
            return f"üìÑ **D·ª±a tr√™n t√†i li·ªáu:** {answer}"
            
        except Exception as e:
            st.error(f"L·ªói khi tr·∫£ l·ªùi c√¢u h·ªèi v·ªõi Local LLM: {str(e)}")
            # Fallback v·ªõi keyword matching
            return self._simple_keyword_answer(question, document_text)
    
    # ƒê·ªÉ t∆∞∆°ng th√≠ch backward, t·∫°o alias
    def answer_question_with_bart(self, question, document_text):
        """Alias cho backward compatibility"""
        return self.answer_question_with_openai(question, document_text)
    
    def _find_relevant_text_for_question(self, question, document_text, max_length=1500):
        """T√¨m ph·∫ßn vƒÉn b·∫£n li√™n quan nh·∫•t v·ªõi c√¢u h·ªèi"""
        # Tr√≠ch xu·∫•t keywords t·ª´ c√¢u h·ªèi
        question_keywords = self._extract_keywords(question.lower())
        
        # Chia document th√†nh c√°c ƒëo·∫°n
        chunks = self.chunk_text(document_text, chunk_size=500, overlap=100)
        
        # T√≠nh ƒëi·ªÉm relevance cho m·ªói chunk
        chunk_scores = []
        for chunk in chunks:
            score = self._calculate_relevance_score(question_keywords, chunk.lower())
            chunk_scores.append((chunk, score))
        
        # S·∫Øp x·∫øp theo ƒëi·ªÉm s·ªë v√† l·∫•y chunks t·ªët nh·∫•t
        chunk_scores.sort(key=lambda x: x[1], reverse=True)
        
        # K·∫øt h·ª£p c√°c chunks t·ªët nh·∫•t trong gi·ªõi h·∫°n max_length
        relevant_text = ""
        current_length = 0
        
        for chunk, score in chunk_scores:
            if score > 0 and current_length + len(chunk) <= max_length:
                relevant_text += chunk + "\n\n"
                current_length += len(chunk)
            if current_length >= max_length:
                break
        
        # N·∫øu kh√¥ng t√¨m th·∫•y g√¨ li√™n quan, l·∫•y ph·∫ßn ƒë·∫ßu document
        if not relevant_text.strip():
            relevant_text = document_text[:max_length]
        
        return relevant_text.strip()
    
    def _extract_keywords(self, text):
        """Tr√≠ch xu·∫•t keywords t·ª´ text"""
        # Lo·∫°i b·ªè stop words ƒë∆°n gi·∫£n
        stop_words = {'l√†', 'g√¨', 'c·ªßa', 'c√≥', 'ƒë∆∞·ª£c', 'n√†y', 'ƒë√≥', 'v√†', 'v·ªõi', 'cho', 't·ª´', 'trong', 'm·ªôt', 'c√°c', 'nh·ªØng', 'khi', 'n√†o', 'ai', '·ªü', 'ƒë√¢u', 'sao', 'nh∆∞', 'th·∫ø', 'what', 'is', 'are', 'the', 'of', 'and', 'or', 'in', 'on', 'at', 'to', 'for', 'with', 'by'}
        
        words = text.replace('?', '').replace(',', '').replace('.', '').split()
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        return keywords
    
    def _calculate_relevance_score(self, keywords, text):
        """T√≠nh ƒëi·ªÉm relevance d·ª±a tr√™n s·ªë l∆∞·ª£ng keywords xu·∫•t hi·ªán"""
        score = 0
        for keyword in keywords:
            score += text.count(keyword)
        return score
    
    def _simple_keyword_answer(self, question, document_text):
        """Fallback method v·ªõi keyword matching ƒë∆°n gi·∫£n"""
        keywords = self._extract_keywords(question.lower())
        
        # T√¨m c√¢u ch·ª©a keywords
        sentences = document_text.split('.')
        relevant_sentences = []
        
        for sentence in sentences:
            sentence_lower = sentence.lower()
            if any(keyword in sentence_lower for keyword in keywords):
                relevant_sentences.append(sentence.strip())
        
        if relevant_sentences:
            # L·∫•y t·ªëi ƒëa 3 c√¢u li√™n quan
            result = '. '.join(relevant_sentences[:3])
            return f"D·ª±a tr√™n t√†i li·ªáu: {result}."
        else:
            return f"Kh√¥ng t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ v·ªÅ '{question}' trong t√†i li·ªáu."
    
    def extract_pdf_page_as_image(self, uploaded_file, page_number: int, dpi: int = 150) -> Optional[Image.Image]:
        """
        Convert trang PDF th√†nh h√¨nh ·∫£nh PIL Image
        
        Args:
            uploaded_file: File PDF ƒë√£ upload
            page_number: S·ªë trang c·∫ßn convert (b·∫Øt ƒë·∫ßu t·ª´ 1)
            dpi: ƒê·ªô ph√¢n gi·∫£i (m·∫∑c ƒë·ªãnh 150 DPI)
            
        Returns:
            PIL Image ho·∫∑c None n·∫øu l·ªói
        """
        try:
            # Reset file pointer
            uploaded_file.seek(0)
            pdf_bytes = uploaded_file.read()
            
            # M·ªü PDF v·ªõi PyMuPDF
            pdf_document = fitz.open(stream=pdf_bytes, filetype="pdf")
            
            # Ki·ªÉm tra page number h·ª£p l·ªá
            if page_number < 1 or page_number > pdf_document.page_count:
                st.error(f"‚ùå Trang {page_number} kh√¥ng t·ªìn t·∫°i. PDF c√≥ {pdf_document.page_count} trang.")
                pdf_document.close()
                return None
            
            # L·∫•y trang (index t·ª´ 0)
            page = pdf_document[page_number - 1]
            
            # T·∫°o matrix v·ªõi DPI c·ª• th·ªÉ
            zoom = dpi / 72.0  # 72 DPI l√† m·∫∑c ƒë·ªãnh
            matrix = fitz.Matrix(zoom, zoom)
            
            # Render trang th√†nh pixmap
            pixmap = page.get_pixmap(matrix=matrix)
            
            # Convert pixmap th√†nh PIL Image
            img_data = pixmap.tobytes("ppm")
            pil_image = Image.open(BytesIO(img_data))
            
            # ƒê√≥ng resources
            pixmap = None
            pdf_document.close()
            
            return pil_image
            
        except Exception as e:
            st.error(f"‚ùå L·ªói khi convert PDF sang ·∫£nh: {str(e)}")
            return None
    
    def get_pdf_page_image_base64(self, uploaded_file, page_number: int, dpi: int = 150) -> Optional[str]:
        """
        Convert trang PDF th√†nh base64 string ƒë·ªÉ hi·ªÉn th·ªã trong Streamlit
        
        Args:
            uploaded_file: File PDF ƒë√£ upload
            page_number: S·ªë trang c·∫ßn convert (b·∫Øt ƒë·∫ßu t·ª´ 1)
            dpi: ƒê·ªô ph√¢n gi·∫£i (m·∫∑c ƒë·ªãnh 150 DPI)
            
        Returns:
            Base64 string ho·∫∑c None n·∫øu l·ªói
        """
        try:
            # L·∫•y PIL Image
            pil_image = self.extract_pdf_page_as_image(uploaded_file, page_number, dpi)
            
            if pil_image is None:
                return None
            
            # Convert PIL Image th√†nh base64
            buffer = BytesIO()
            pil_image.save(buffer, format="PNG")
            img_bytes = buffer.getvalue()
            img_base64 = base64.b64encode(img_bytes).decode()
            
            return img_base64
            
        except Exception as e:
            st.error(f"‚ùå L·ªói khi t·∫°o base64 image: {str(e)}")
            return None
    
    def get_pdf_page_count_with_pymupdf(self, uploaded_file) -> int:
        """
        L·∫•y s·ªë trang PDF b·∫±ng PyMuPDF (fallback method)
        
        Args:
            uploaded_file: File PDF ƒë√£ upload
            
        Returns:
            S·ªë trang c·ªßa PDF
        """
        try:
            uploaded_file.seek(0)
            pdf_bytes = uploaded_file.read()
            
            # M·ªü PDF v·ªõi PyMuPDF
            pdf_document = fitz.open(stream=pdf_bytes, filetype="pdf")
            page_count = pdf_document.page_count
            pdf_document.close()
            
            return page_count
            
        except Exception as e:
            st.error(f"‚ùå L·ªói khi ƒë·∫øm trang PDF v·ªõi PyMuPDF: {str(e)}")
            return 0
    
    def get_pdf_image_dimensions(self, uploaded_file, page_number: int) -> tuple:
        """
        L·∫•y k√≠ch th∆∞·ªõc g·ªëc c·ªßa trang PDF
        
        Args:
            uploaded_file: File PDF ƒë√£ upload
            page_number: S·ªë trang c·∫ßn ki·ªÉm tra
            
        Returns:
            Tuple (width, height) ho·∫∑c (0, 0) n·∫øu l·ªói
        """
        try:
            uploaded_file.seek(0)
            pdf_bytes = uploaded_file.read()
            
            pdf_document = fitz.open(stream=pdf_bytes, filetype="pdf")
            
            if page_number < 1 or page_number > pdf_document.page_count:
                pdf_document.close()
                return (0, 0)
            
            page = pdf_document[page_number - 1]
            rect = page.rect
            
            pdf_document.close()
            
            return (int(rect.width), int(rect.height))
            
        except Exception as e:
            st.error(f"‚ùå L·ªói khi l·∫•y k√≠ch th∆∞·ªõc PDF: {str(e)}")
            return (0, 0)
